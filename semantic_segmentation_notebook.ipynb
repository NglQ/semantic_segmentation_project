{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 17:52:23.622847: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-09 17:52:23.860486: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-09 17:52:24.607932: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/angelo/anaconda3/lib/python3.9/site-packages/cv2/../../../../lib:\n",
      "2023-09-09 17:52:24.608016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/angelo/anaconda3/lib/python3.9/site-packages/cv2/../../../../lib:\n",
      "2023-09-09 17:52:24.608022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import signal\n",
    "import random\n",
    "\n",
    "# Algebra\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Neural Networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "from keras.layers import Input, SeparableConv2D, BatchNormalization, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers import concatenate, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import ndimage\n",
    "\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell defines the functions used to load the data from the dataset. The dataset is composed of images and masks. The masks are one-hot encoded. Using PIL instead of pyplot to load the images, the resulting range of the images is [0, 6] instead of [0, 1]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_data_from_names(root_dir: str, fnames: list, shape=(1000, 1000), mask=False, resize_image=True) -> np.array:\n",
    "    # Given the root path and a list of file names as input, return the dataset\n",
    "    # array.\n",
    "    images = []\n",
    "\n",
    "    for idx, img_name in enumerate(fnames):\n",
    "        x = Image.open(os.path.join(root_dir, img_name))\n",
    "        if resize_image:\n",
    "            x = x.resize(shape)\n",
    "        if not mask:\n",
    "            x = np.array(x)/255.\n",
    "        images.append(np.array(x))\n",
    "\n",
    "        if idx%100 == 99:\n",
    "            print(f\"Processed {idx+1} images.\")\n",
    "\n",
    "    value = np.array(images) if not mask else one_hot_encoder(np.array(images))\n",
    "    return value\n",
    "\n",
    "\n",
    "def one_hot_encoder(y):\n",
    "    unique_values = np.unique(y)\n",
    "    one_hot_encoding = np.zeros(y.shape + (len(unique_values),), dtype=np.int8)\n",
    "    for i, value in enumerate(unique_values):\n",
    "        one_hot_encoding[(y == value), i] = 1\n",
    "    return one_hot_encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having just 600 images, I decided to create a brand-new dataset starting from the previous one. Since I had some troubles fitting all the dataset into my RAM, I got 256X 256 patches from the original images. Each patch has been chosen getting the part of the image having the largest variance in the corresponding mask.\n",
    "The variance has been computed using a 5X5 window. The patches have been chosen in order to have a minimum overlapping area between them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def overlapping_area(l1, r1, l2, r2):\n",
    "    x = 0\n",
    "    y = 1\n",
    "    ''' Length of intersecting part i.e\n",
    "        start from max(l1[x], l2[x]) of\n",
    "        x-coordinate and end at min(r1[x],\n",
    "        r2[x]) x-coordinate by subtracting\n",
    "        start from end we get required\n",
    "        lengths '''\n",
    "    x_dist = abs(min(r1[x], r2[x]) - max(l1[x], l2[x]))\n",
    "\n",
    "    y_dist = abs(min(r1[y], r2[y]) - max(l1[y], l2[y]))\n",
    "\n",
    "    area_i = 0\n",
    "    if x_dist > 0 and y_dist > 0:\n",
    "        area_i = x_dist * y_dist\n",
    "\n",
    "    return area_i\n",
    "\n",
    "\n",
    "def create_training_set(target_max=True):\n",
    "\n",
    "    rows, cols = 128, 128\n",
    "    win_rows, win_cols = 5, 5\n",
    "    threshold = .5 * (2 * rows * 2 * cols) if target_max else .2 * (2 * rows * 2 * cols)\n",
    "\n",
    "    target_path_annotation_train = './ign_new/annotations/training/'\n",
    "    target_path_images_train = './ign_new/images/training/'\n",
    "\n",
    "    image_names = os.listdir('./ign/images/training')\n",
    "    mask_names = os.listdir('./ign/annotations/training')\n",
    "\n",
    "    image_names.sort()\n",
    "    mask_names.sort()\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    x = load_data_from_names('./ign/images/training', image_names, shape=(256, 256), resize_image=False)\n",
    "    y = load_data_from_names('./ign/annotations/training', mask_names, shape=(256, 256), mask=True, resize_image=False)\n",
    "    for ((img, img_name), (mask, mask_name)) in zip(zip(x, image_names), zip(y, mask_names)):\n",
    "        win_mean = ndimage.uniform_filter(np.argmax(mask, axis=-1), (win_rows, win_cols))\n",
    "        win_sqr_mean = ndimage.uniform_filter(np.argmax(mask, axis=-1)**2, (win_rows, win_cols))\n",
    "        win_var = win_sqr_mean - win_mean**2\n",
    "\n",
    "        mx_place = np.where(win_var == win_var.max()) if target_max else np.where(win_var == win_var.min())\n",
    "\n",
    "        old_patches = []\n",
    "\n",
    "        zip_var = zip(mx_place[0], mx_place[1]) if target_max else zip(mx_place[0][:250], mx_place[1][:250])\n",
    "\n",
    "        for k, (i, j) in enumerate(zip_var):\n",
    "            x_min = i-rows\n",
    "            x_max = i+rows\n",
    "            y_min = j-cols\n",
    "            y_max = j+cols\n",
    "            if x_min < 0:\n",
    "                x_min = 0\n",
    "                x_max = 2*rows\n",
    "            if x_max > 1000:\n",
    "                x_max = 1000\n",
    "                x_min = 1000 - 2*rows\n",
    "            if y_min < 0:\n",
    "                y_min = 0\n",
    "                y_max = 2*cols\n",
    "            if y_max > 1000:\n",
    "                y_max = 1000\n",
    "                y_min = 1000 - 2*cols\n",
    "\n",
    "            win_var_1 = np.argmax(mask, axis=-1)[x_min:x_max, y_min:y_max]\n",
    "            img_1_var = img[x_min:x_max, y_min:y_max]*255\n",
    "\n",
    "            if old_patches == []:\n",
    "                cv2.imwrite(f'{target_path_images_train}{img_name[:img_name.find(\".\")]}_{k}.png', img_1_var)\n",
    "                cv2.imwrite(f'{target_path_annotation_train}{mask_name[:mask_name.find(\".\")]}_{k}.png', win_var_1)\n",
    "                old_patches.append((k, (x_min, x_max), (y_min, y_max)))\n",
    "            else:\n",
    "                a = [overlapping_area((x_min, y_min), (x_max, y_max), (old_x_min, old_y_min), (old_x_max, old_y_max)) < threshold\n",
    "                     for (kk, (old_x_min, old_x_max), (old_y_min, old_y_max)) in old_patches]\n",
    "                if all(a):\n",
    "                    cv2.imwrite(f'{target_path_images_train}{img_name[:img_name.find(\".\")]}_{k}.png', img_1_var)\n",
    "                    cv2.imwrite(f'{target_path_annotation_train}{mask_name[:mask_name.find(\".\")]}_{k}.png', win_var_1)\n",
    "                    old_patches.append((k, (x_min, x_max), (y_min, y_max)))\n",
    "\n",
    "        if cnt % 10 == 9:\n",
    "            print(cnt, 'out of', len(image_names))\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def create_test_set():\n",
    "\n",
    "    target_path_annotation_val = './ign_new/annotations/validation/'\n",
    "    target_path_images_val = './ign_new/images/validation/'\n",
    "\n",
    "    image_names = os.listdir('./ign/images/validation')\n",
    "    mask_names = os.listdir('./ign/annotations/validation')\n",
    "\n",
    "    image_names.sort()\n",
    "    mask_names.sort()\n",
    "\n",
    "    x = load_data_from_names('./ign/images/validation', image_names, shape=(256, 256))\n",
    "    y = load_data_from_names('./ign/annotations/validation', mask_names, shape=(256, 256), mask=True)\n",
    "\n",
    "    for ((img, img_name), (mask, mask_name)) in zip(zip(x, image_names), zip(y, mask_names)):\n",
    "        cv2.imwrite(f'{target_path_images_val}{img_name}', img*255)\n",
    "        cv2.imwrite(f'{target_path_annotation_val}{mask_name}', np.argmax(mask, axis=-1))\n",
    "\n",
    "\n",
    "def create_dataset():\n",
    "    create_training_set()\n",
    "    create_test_set()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following snippet of code define the metrics used to evaluate the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    smooth = 0.0001\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_multilabel(y_true, y_pred, numLabels=7):\n",
    "    dice = 0\n",
    "    y_true = y_true.numpy()\n",
    "    y_pred = y_pred.numpy()\n",
    "    for index in range(numLabels):\n",
    "        dice += dice_coef(y_true[:, :, :, index], y_pred[:, :, :, index])\n",
    "    return dice\n",
    "\n",
    "def np_dice_coef_multilabel(y_true, y_pred, numLabels=7):\n",
    "    dice = 0\n",
    "    for index in range(numLabels):\n",
    "        dice += dice_coef(y_true[:, :, :, index], y_pred[:, :, :, index])\n",
    "    return dice/numLabels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following model is a modified version of the MobileUNet model. The main differences are the use of separable convolutional layers instead of the standard ones and the use of batch normalization layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def mod_mobileunet(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    conv1 = SeparableConv2D(64, 3, activation='elu', padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = SeparableConv2D(64, 3, activation='elu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = SeparableConv2D(128, 3, activation='elu', padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = SeparableConv2D(128, 3, activation='elu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = SeparableConv2D(256, 3, activation='elu', padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = SeparableConv2D(256, 3, activation='elu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "\n",
    "    conv8 = Conv2DTranspose(128, 3, strides=(2, 2), activation='elu', padding='same')(conv3)\n",
    "    cat8 = concatenate([conv2, conv8], axis=3)\n",
    "    conv8 = SeparableConv2D(128, 3, activation='elu', padding='same')(cat8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = SeparableConv2D(128, 3, activation='elu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "\n",
    "    conv9 = Conv2DTranspose(64, 3, strides=(2, 2), activation='elu', padding='same')(conv8)\n",
    "    cat9 = concatenate([conv1, conv9], axis=3)\n",
    "    conv9 = SeparableConv2D(64, 3, activation='elu', padding='same')(cat9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = SeparableConv2D(64, 3, activation='elu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(7, 1, activation='softmax')(conv9)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy',\n",
    "                  metrics=[dice_coef_multilabel], run_eagerly=True)\n",
    "\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This callback is designed to display the results of the model during the training phase. It shows the original image, the ground truth mask and the predicted mask that is made on an image drawn from the validation set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DisplayCallback(ks.callbacks.Callback):\n",
    "    def __init__(self, epoch_interval=None, test_images=None, test_masks=None, BATCH_SIZE=None):\n",
    "        self.epoch_interval = epoch_interval\n",
    "        self.test_images = test_images\n",
    "        self.test_masks = test_masks\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.epoch_interval and epoch % self.epoch_interval == 0:\n",
    "            pred_masks = self.model.predict(self.test_images)\n",
    "            pred_masks = tf.math.argmax(pred_masks, axis=-1)\n",
    "            pred_masks = np.argmax(pred_masks[..., tf.newaxis], axis=-1)\n",
    "\n",
    "            # Randomly select an image from the test batch\n",
    "            random_index = random.randint(0, self.BATCH_SIZE - 1)\n",
    "            random_image = self.test_images[random_index]\n",
    "            random_pred_mask = pred_masks[random_index]\n",
    "            random_true_mask = np.argmax(self.test_masks[random_index], axis=-1)\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
    "            ax[0].imshow(random_image)\n",
    "            ax[0].set_title(f\"Image: {epoch:03d}\")\n",
    "\n",
    "            ax[1].imshow(random_true_mask)\n",
    "            ax[1].set_title(f\"Ground Truth Mask: {epoch:03d}\")\n",
    "\n",
    "            ax[2].imshow(random_pred_mask)\n",
    "            ax[2].set_title(\n",
    "                f\"Predicted Mask: {epoch:03d}\",\n",
    "            )\n",
    "\n",
    "            plt.show()\n",
    "            plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training phase is performed by the following cell. The model is trained for 300 epochs. The training is stopped if the validation dice coefficient does not improve for 15 epochs. The model is saved every time the validation dice coefficient improves.\n",
    "The training is performed on a subset of the dataset composed of 1000 images split in training set and validation in a 90%/10% ratio.\n",
    "A panic button-like feature has been implemented in order to save the model if the training is interrupted using ctrl+C."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "CNN_model = None\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    if CNN_model is not None:\n",
    "        CNN_model.save('./saved_models/semantic_segmentation_panic_new_data.h5')\n",
    "        print('Model saved', flush=True)\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "\n",
    "def train_model(pretrained_model=None):  # pretrained_model is the path to the pretrained model to be used\n",
    "    image_names = os.listdir('./ign_new/images/training')[:1000]\n",
    "    mask_names = os.listdir('./ign_new/annotations/training')[:1000]\n",
    "\n",
    "    image_names.sort()\n",
    "    mask_names.sort()\n",
    "\n",
    "    x = load_data_from_names('./ign_new/images/training', image_names, shape=(256, 256))\n",
    "    y = load_data_from_names('./ign_new/annotations/training', mask_names, shape=(256, 256), mask=True)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    global CNN_model\n",
    "    if pretrained_model:\n",
    "        CNN_model = ks.models.load_model(pretrained_model, custom_objects={'dice_coef_multilabel': dice_coef_multilabel})\n",
    "        CNN_model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=[dice_coef_multilabel], run_eagerly=True)\n",
    "    else:\n",
    "        CNN_model = mod_mobileunet(input_size=(256, 256, 3))\n",
    "\n",
    "    print(CNN_model.summary())\n",
    "\n",
    "    # Set hyperparameters\n",
    "    BATCH_SIZE = 8\n",
    "    N_EPOCHS = 300\n",
    "\n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    cBack = tf.keras.callbacks.EarlyStopping(monitor='val_dice_coef_multilabel', patience=15, mode='max', restore_best_weights=True)\n",
    "    checkPointCb = tf.keras.callbacks.ModelCheckpoint(filepath=\"./saved_models/semantic_segmentation_new_data.h5\",\n",
    "                                                      monitor='val_dice_coef_multilabel', mode='max', save_best_only=True)\n",
    "\n",
    "    display_callback = DisplayCallback(epoch_interval=1, test_images=x_val, test_masks=y_val, BATCH_SIZE=BATCH_SIZE)\n",
    "    # dice_metric_callback = Metrics(7, CNN_model)\n",
    "\n",
    "    # Training\n",
    "    hist = CNN_model.fit(x_train, y_train,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         epochs=N_EPOCHS,\n",
    "                         validation_data=(x_val, y_val),\n",
    "                         callbacks=[tensorboard_callback, cBack, checkPointCb])  # , display_callback])\n",
    "\n",
    "    CNN_model.save('./saved_models/semantic_segmentation_new_data.h5')\n",
    "\n",
    "    return CNN_model, hist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The test set has 200 images which are the down-sampled version of the original ones in order to suit the requirements in terms of . The following cell computes the mean dice coefficient on the test set. MOreover, it displays the original image, the ground truth mask and the predicted mask for each image in the test set for each of the test set images.\n",
    "In order to see further insights on the model, the logs have been saved in a folder called 'logs' that can be visualized by tensorboard."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    image_names = os.listdir('./ign_new/images/validation')\n",
    "    mask_names = os.listdir('./ign_new/annotations/validation')\n",
    "\n",
    "    image_names.sort()\n",
    "    mask_names.sort()\n",
    "\n",
    "    x = load_data_from_names('./ign_new/images/validation', image_names, shape=(256, 256))\n",
    "    y = load_data_from_names('./ign_new/annotations/validation', mask_names, shape=(256, 256), mask=True)\n",
    "    model = ks.models.load_model('./saved_models/semantic_segmentation_analyze_me.h5', custom_objects={'dice_coef_multilabel': dice_coef_multilabel})\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=[dice_coef_multilabel], run_eagerly=True)\n",
    "\n",
    "    predictions = model.predict(x)\n",
    "\n",
    "    print('Mean dice coefficient multi-label', np_dice_coef_multilabel(y, predictions))\n",
    "\n",
    "    for image, pred, y_true in zip(x, predictions, y):\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
    "        ax[0].imshow(image)\n",
    "\n",
    "        ax[1].imshow(np.argmax(y_true, axis=-1))\n",
    "\n",
    "        ax[2].imshow(np.argmax(pred, axis=-1))\n",
    "\n",
    "        plt.show()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform the training on the model and plot the results in terms of loss and dice_coef_multilabel. [[ TODO: comment on the results ]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images.\n",
      "Processed 200 images.\n",
      "Processed 300 images.\n",
      "Processed 400 images.\n",
      "Processed 500 images.\n",
      "Processed 600 images.\n",
      "Processed 700 images.\n",
      "Processed 800 images.\n",
      "Processed 900 images.\n",
      "Processed 1000 images.\n",
      "Processed 100 images.\n",
      "Processed 200 images.\n",
      "Processed 300 images.\n",
      "Processed 400 images.\n",
      "Processed 500 images.\n",
      "Processed 600 images.\n",
      "Processed 700 images.\n",
      "Processed 800 images.\n",
      "Processed 900 images.\n",
      "Processed 1000 images.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_10 (Separable  (None, 256, 256, 64  283        ['input_2[0][0]']                \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 256, 256, 64  256        ['separable_conv2d_10[0][0]']    \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_11 (Separable  (None, 256, 256, 64  4736       ['batch_normalization_9[0][0]']  \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 256, 256, 64  256        ['separable_conv2d_11[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 64  0          ['batch_normalization_10[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_12 (Separable  (None, 128, 128, 12  8896       ['max_pooling2d_2[0][0]']        \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_12[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_13 (Separable  (None, 128, 128, 12  17664      ['batch_normalization_11[0][0]'] \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_13[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 128)  0          ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 64, 64, 256)  34176      ['max_pooling2d_3[0][0]']        \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 64, 64, 256)  1024       ['separable_conv2d_14[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 64, 64, 256)  68096      ['batch_normalization_13[0][0]'] \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 64, 64, 256)  1024       ['separable_conv2d_15[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 12  295040     ['batch_normalization_14[0][0]'] \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 25  0           ['batch_normalization_12[0][0]', \n",
      "                                6)                                'conv2d_transpose_2[0][0]']     \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 128, 128, 12  35200      ['concatenate_2[0][0]']          \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_16[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 128, 128, 12  17664      ['batch_normalization_15[0][0]'] \n",
      " Conv2D)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 128, 128, 12  512        ['separable_conv2d_17[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 64  73792      ['batch_normalization_16[0][0]'] \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 256, 12  0           ['batch_normalization_10[0][0]', \n",
      "                                8)                                'conv2d_transpose_3[0][0]']     \n",
      "                                                                                                  \n",
      " separable_conv2d_18 (Separable  (None, 256, 256, 64  9408       ['concatenate_3[0][0]']          \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 256, 256, 64  256        ['separable_conv2d_18[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_19 (Separable  (None, 256, 256, 64  4736       ['batch_normalization_17[0][0]'] \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 256, 256, 7)  455         ['separable_conv2d_19[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 575,010\n",
      "Trainable params: 572,578\n",
      "Non-trainable params: 2,432\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 17:03:30.107344: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 707788800 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "CNN_model, hist = train_model()\n",
    "loss_history = hist.history['loss']\n",
    "val_loss_history = hist.history['val_loss']\n",
    "\n",
    "acc_history = hist.history['dice_coef_multilabel']\n",
    "val_acc_history = hist.history['val_dice_coef_multilabel']\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.plot(val_loss_history)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Loss', 'Val Loss'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acc_history)\n",
    "plt.plot(val_acc_history)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['dice_coef_multilabel', 'Val dice_coef_multilabel'])\n",
    "plt.title('dice_coef_multilabel')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform the test on the model [[ TODO: comment on the results ]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
